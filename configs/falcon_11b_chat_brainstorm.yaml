model_name: falcon2_11b_chat_brainstorm_c8000
base_model: tiiuae/falcon-11B
#model_family: llama  # if unspecified will use AutoModelForCausalLM/AutoTokenizer
model_context_window: 8192  # if unspecified will use tokenizer.model_max_length
data:
  type: vicuna
  dataset: DevQuasar/brainstorm_vicuna_1k  # HuggingFace hub
  user_header: "### HUMAN:\n"
  response_header: "### RESPONSE:\n"
lora:
  r: 8
  lora_alpha: 32
  target_modules:  # modules for which to train lora adapters
  - query_key_value
  - dense
  - dense_h_to_4h
  - dense_4h_to_h
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
trainer:
  batch_size: 1
  gradient_accumulation_steps: 4
  warmup_steps: 100
  num_train_epochs: 50
  learning_rate: 0.0002  # 2e-4
  logging_steps: 20
trainer_output_dir: trainer_outputs_falcon/
model_output_dir: models/  # model saved in {model_output_dir}/{model_name}
